{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":130383,"databundleVersionId":15646669}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pathlib\n\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import compose, dummy, impute, metrics, pipeline, preprocessing\n\n\nDATA_DIR = pathlib.Path(\"/kaggle/input/rainfall-probability-cs-209-spring-2026\")\nRANDOM_STATE = np.random.RandomState(42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the Data","metadata":{}},{"cell_type":"code","source":"%%bash\n\nls /kaggle/input/rainfall-probability-cs-209-spring-2026","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n\ncat /kaggle/input/rainfall-probability-cs-209-spring-2026/train.csv | head -n 5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_name = \"rainfall\"\n\ntrain_df = pd.read_csv(\n    DATA_DIR / \"train.csv\",\n    index_col=\"id\",\n)\ntrain_features_df = train_df.drop(label_name, axis=\"columns\")\ntrain_labels = train_df.loc[:, label_name]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n\ncat /kaggle/input/rainfall-probability-cs-209-spring-2026/test.csv | head -n 5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_features_df = pd.read_csv(\n    DATA_DIR / \"test.csv\",\n    index_col=\"id\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare the Data for ML\n\n## Create Data Preparation Pipelines\n\nThe `pipeline.Pipeline` class in **Scikit-Learn** is a **tool for chaining multiple data processing and modeling steps together** into a single object. Its main purpose is to **streamline preprocessing and model training**, while **preventing data leakage** during cross-validation or testing.\n\n### Key points:\n\n1. **Sequence of steps:**\n   Each step has a name and a transformer or estimator:\n\n   ```python\n   ml_pipeline = pipeline.Pipeline([\n       (\"simple_imputer\", impute.SimpleImputer()),             # preprocessing step 1\n       ('standard_scaler', preprocessing.StandardScaler()),    # preprocessing step 2\n       ('linear_regression', linear_model.LinearRegression())  # final estimator\n   ])\n   ```\n\n2. **Fit and transform automatically:**\n\n   * `ml_pipeline.fit(X_train, y_train)` applies all transformations in order using each transformer's `fit_transform` method and then trains the final estimator using the `fit` method.\n   * `ml_pipeline.predict(X_test)` applies the same transformations in order to new data using each transformer's `transform` method and then uses the final estimator's `predict` method to make predictions.\n\n3. **Prevents leakage during cross-validation:**\n   When used with cross-validation routines such as `model_selection.cross_val_score`, or `model_selection.GridSearchCV`, `pipeline.Pipeline` objects are fit to the training folds avoiding any leakage from the validation data.\n\n4. **Hyperparameter tuning parameter naming convention:**\n   You can tune parameters of any step in the pipeline with `model_selection.GridSearchCV` (or similar routine) using the syntax `\"step_name__parameter\"`.\n","metadata":{}},{"cell_type":"code","source":"pipeline.Pipeline?","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Categorical Features\n","metadata":{}},{"cell_type":"markdown","source":"### Handling Missing Values\n\n`impute.SimpleImputer` is a preprocessing tool that **fills in missing values** in a dataset.\n\n* During `.fit()`, it **learns a replacement value** from the training data (e.g., the **mean**, **median**, or **most frequent** value in each column).\n* During `.transform()`, it **replaces missing entries** (like `NaN`) using those learned values.\n\nIt’s commonly used inside a `pipeline.Pipeline` to avoid data leakage and ensure consistent preprocessing during cross-validation.\n","metadata":{}},{"cell_type":"code","source":"impute.SimpleImputer?","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Encoding Ordered Categories\n\n`preprocessing.OrdinalEncoder` converts **categorical (string) features** into **integer-coded categories**.\n\nFor example:\n\n* `\"red\", \"green\", \"blue\"`\n  → `2, 1, 0` (or some learned ordering)\n\nIt assigns each category in each feature an **integer index** based on what it sees during `fit()`.\n\nBy mapping categories to integers, `preprocessing.OrdinalEncoder` implies an numerical ordering to the original categories (e.g., `blue < green < red`). It’s often best for:\n\n* **tree-based models**, or\n* categorical variables that are **truly ordered**.\n\nFor unordered categories, `preprocessing.OneHotEncoder` is usually safer.\n","metadata":{}},{"cell_type":"code","source":"preprocessing.OrdinalEncoder?","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features_preprocessing = pipeline.Pipeline(\n    steps=[\n        (\n            \"simple_imputer\",\n            impute.SimpleImputer(\n                strategy=\"most_frequent\",\n            ),\n        ),\n        (\n            \"ordinal_encoder\",\n            preprocessing.OrdinalEncoder(\n                categories=[\n                    range(1, 365 + 1)\n                ],\n                handle_unknown=\"error\",\n            )\n        )\n    ],\n    memory=None,\n    verbose=False,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features_preprocessing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Numerical Features","metadata":{}},{"cell_type":"markdown","source":"### Standardizing Features\n\n`preprocessing.StandardScaler` **standardizes numerical features** by transforming each feature (column) to have:\n\n* **mean = 0**\n* **standard deviation = 1**\n\nIt does this by **learning** the mean and std from the training data during `fit()`, then applying:\n\n$$ x' = \\frac{x - \\mu}{\\sigma} $$\n\nduring `transform()`.\n\nThis is especially useful for models that are sensitive to feature scale (e.g., SGD, SVMs, k-NN, neural nets).\n","metadata":{}},{"cell_type":"code","source":"preprocessing.StandardScaler?","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_features_preprocessing = pipeline.Pipeline(\n    steps=[\n        (\n            \"simple_imputer\",\n            impute.SimpleImputer(\n                strategy=\"mean\",\n            )\n        ),\n        (\n            \"standard_scaler\",\n            preprocessing.StandardScaler(\n                with_mean=True,\n                with_std=True,\n            )\n        )\n    ],\n    memory=None,\n    verbose=False,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_features_preprocessing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Combine Feature Preprocessing Pipelines","metadata":{}},{"cell_type":"markdown","source":"### Column-based Transformations\n\n`compose.ColumnTransformer` lets you apply **different preprocessing pipelines to different columns** of your dataset in a single, clean step.\n\nFor example, you can:\n\n* **impute + scale** numeric columns, and\n* **impute + one-hot encode** categorical columns,\n\nthen it **combines all transformed outputs into one final feature matrix** that you can feed into a model (often inside a `pipeline.Pipeline`).\n","metadata":{}},{"cell_type":"code","source":"compose.ColumnTransformer?","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_preprocessing = compose.ColumnTransformer(\n    transformers=[\n        (\n            \"categorical_features\",\n            categorical_features_preprocessing,\n            [\n                \"day\",\n            ]\n        ),\n        (\n            \"numerical_features\",\n            numerical_features_preprocessing,\n            [\n                \"pressure\",\n                \"maxtemp\",\n                \"temparature\",\n                \"mintemp\",\n                \"dewpoint\",\n                \"humidity\",\n                \"cloud\",\n                \"sunshine\",\n                \"winddirection\",\n                \"windspeed\",\n            ]\n        ),\n    ],  \n    force_int_remainder_cols=False,\n    remainder=\"drop\",\n    n_jobs=2,\n    verbose=False,\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_preprocessing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Manually Preprocessing Features","metadata":{}},{"cell_type":"code","source":"processed_train_features_df = features_preprocessing.fit_transform(train_features_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_train_features_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_train_features_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Benchmark Model","metadata":{}},{"cell_type":"markdown","source":"## Dummy Classifiers\n\n`dummy.DummyClassifier` is a **baseline classifier** that makes predictions using **simple, non-learning rules** instead of actually training on patterns in the data.\n\nCommon strategies include:\n\n* **`most_frequent`**: always predicts the most common class\n* **`prior`**: predicts according to class proportions\n* **`stratified`**: predicts randomly but respecting class proportions\n* **`uniform`**: predicts completely at random\n* **`constant`**: always predicts a user-specified class\n\nIt’s mainly used to check whether your real model is doing **better than a trivial baseline**.\n","metadata":{}},{"cell_type":"code","source":"dummy.DummyClassifier?","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using Manually Preprocessed Features","metadata":{}},{"cell_type":"code","source":"dummy_classifier = dummy.DummyClassifier(\n    strategy=\"prior\",\n    random_state=RANDOM_STATE,\n)\n\n_ = dummy_classifier.fit(\n    processed_train_features_df,\n    train_labels\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Combine Feature Preprocessing with a Model","metadata":{}},{"cell_type":"code","source":"classifier_pipeline = pipeline.Pipeline(\n    steps=[\n        (\"features_preprocessing\", features_preprocessing),\n        (\"dummy_classifier\", dummy_classifier)\n    ]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classifier_pipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_ = classifier_pipeline.fit(\n    train_features_df,\n    train_labels\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save a Trained Pipeline","metadata":{}},{"cell_type":"code","source":"_ = joblib.dump(classifier_pipeline, \"dummy-classifier-pipeline.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n\nls -lh","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submit Predictions","metadata":{}},{"cell_type":"code","source":"%%bash\n\ncat /kaggle/input/rainfall-probability-cs-209-spring-2026/sample_submission.csv | head -n 5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Model Predictions","metadata":{}},{"cell_type":"code","source":"loaded_classifier_pipeline = joblib.load(\"dummy-classifier-pipeline.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loaded_classifier_pipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_rainfall_probas = loaded_classifier_pipeline.predict_proba(\n    test_features_df\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create a Submission File","metadata":{}},{"cell_type":"code","source":"_ = (\n    pd.read_csv(\n        DATA_DIR / \"sample_submission.csv\",\n        index_col=\"id\"\n    ).assign(\n        rainfall=predicted_rainfall_probas[:, 1]\n    ).to_csv(\n        \"submission.csv\",\n        index=True\n    )\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n\ncat submission.csv | head -n 5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}